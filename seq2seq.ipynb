{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing Sequence to Sequence model on a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence model has two parts an encoder part which encodes the variable lenght input, say a source language, to a fixed size representation called the context, and a decoder part which decodes this representation to a variable lenght representation, say to a target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoder Part\n",
    "\n",
    "class seq2seq_encoder:\n",
    "    \n",
    "    def __init__(self,n_hidden,n_layers,batch_size,time_steps,input_size):\n",
    "        context_size= input_size\n",
    "        self.w= tf.Variable(tf.truncated_normal(shape= [input_size,\n",
    "                                                        n_hidden],\n",
    "                                               dtype= tf.float32,\n",
    "                                               stddev= 0.01))\n",
    "        self.b= tf.Variable(tf.constant(0.01, shape= [n_hidden]))\n",
    "        self.cell= tf.contrib.rnn.GRUCell(n_hidden)\n",
    "        self.input_size= input_size\n",
    "        self.context_size= input_size\n",
    "        self.w_cntxt= tf.Variable(tf.truncated_normal(shape= [n_hidden,\n",
    "                                                              input_size],\n",
    "                                               dtype= tf.float32,\n",
    "                                               stddev= 0.01))\n",
    "        self.b_cntxt= tf.Variable(tf.constant(0.01, shape= [input_size]))\n",
    "        self.n_hidden= n_hidden\n",
    "        self.batch_size= batch_size\n",
    "        self.time_steps= time_steps\n",
    "        \n",
    "    # shape of encoder_input must be [batch_size,time_steps,input_dim] \n",
    "    # encode method returns context and state of each batch.\n",
    "    def encode(self,encoder_input):\n",
    "        X_in= tf.matmul(tf.reshape(encoder_input,(-1,self.input_size)),\n",
    "                       self.w)+self.b\n",
    "        X_in= tf.reshape(X_in, (-1,self.time_steps,self.n_hidden))\n",
    "        init_state= self.cell.zero_state(self.batch_size,\n",
    "                                         dtype= tf.float32)\n",
    "        output,state= tf.nn.dynamic_rnn(self.cell,X_in,\n",
    "                                        initial_state= init_state,\n",
    "                                       scope= 'encoder')\n",
    "        output= tf.unstack(tf.transpose(output, (1,0,2)))\n",
    "        context= tf.matmul(output[-1],self.w_cntxt)+self.b_cntxt\n",
    "        \n",
    "        return (context,state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decoder Part\n",
    "\n",
    "class seq2seq_decoder:\n",
    "    \n",
    "    def __init__(self,n_hidden,n_layers,batch_size,time_steps,input_size):\n",
    "        self.w= tf.Variable(tf.truncated_normal(shape= [input_size,n_hidden],\n",
    "                                               dtype= tf.float32,\n",
    "                                               stddev= 0.01))\n",
    "        self.b= tf.Variable(tf.constant(0.01, shape= [n_hidden]))\n",
    "        self.cell= tf.contrib.rnn.GRUCell(n_hidden)\n",
    "        self.input_size= input_size\n",
    "        self.w_out= tf.Variable(tf.truncated_normal(shape= [n_hidden,input_size],\n",
    "                                               dtype= tf.float32,\n",
    "                                               stddev= 0.01))\n",
    "        self.b_out= tf.Variable(tf.constant(0.01, shape= [input_size]))\n",
    "        self.time_steps= time_steps\n",
    "        self.n_layers= n_layers\n",
    "        self.n_hidden= n_hidden\n",
    "    \n",
    "    # decode methods returns the decoded output for each batch at \n",
    "    # each time steps.\n",
    "    def decode(self,labels,context,state):\n",
    "        c= tf.reshape(context, (-1,1,self.input_size))\n",
    "        decoder_input= tf.concat((c,labels), axis= 1)\n",
    "        decoder_input= decoder_input[:,:-1]\n",
    "        X_in= tf.matmul(tf.reshape(decoder_input,\n",
    "                                   (-1,self.input_size)), self.w)+self.b\n",
    "        X_in= tf.reshape(X_in, (-1,time_steps,n_hidden))\n",
    "        output,state= tf.nn.dynamic_rnn(self.cell,X_in,\n",
    "                                        initial_state= state,\n",
    "                                       scope= 'decoder')\n",
    "        output= tf.matmul(tf.reshape(output, \n",
    "                                     (-1,self.n_hidden)),\n",
    "                          self.w_out)+self.b_out\n",
    "        output= tf.reshape(output, (-1,self.time_steps,self.input_size))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "\n",
    "time_steps= 20\n",
    "batch_size= 128\n",
    "input_size= 5\n",
    "output_size= input_size\n",
    "n_hidden= 64\n",
    "num_layers= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I will generate a dataset for the model. Here I will encode this dataset using encoder part and then recover the original data using decoder part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x= np.random.rand(batch_size*100,time_steps,input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_input= tf.placeholder(shape= [None,time_steps,input_size], dtype= tf.float32)\n",
    "decoder_input= tf.placeholder(shape= [None,time_steps,input_size], dtype= tf.float32)\n",
    "labels= tf.placeholder(shape= [None,time_steps,output_size], dtype= tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder= seq2seq_encoder(n_hidden,num_layers,\n",
    "                         batch_size,time_steps,input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context,state= encoder.encode(encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder= seq2seq_decoder(n_hidden,num_layers,\n",
    "                         batch_size,time_steps,input_size)\n",
    "y_= decoder.decode(labels,context,state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss= tf.losses.mean_squared_error(labels,y_)\n",
    "optimize= tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init= tf.global_variables_initializer()\n",
    "sess= tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(600):\n",
    "    total_loss= 0\n",
    "    for j in range(90):\n",
    "        enc_batch= x[batch_size*j:batch_size*(j+1)]\n",
    "        labels_batch= enc_batch\n",
    "        \n",
    "        cost,_= sess.run([loss,optimize], feed_dict= {\n",
    "            encoder_input:enc_batch, labels: labels_batch})\n",
    "        total_loss+= cost\n",
    "        print (i,j,cost)\n",
    "    print (i,total_loss/90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss for the test part is 0.021\n"
     ]
    }
   ],
   "source": [
    "total_loss= 0\n",
    "for j in range(90,100):\n",
    "    enc_batch= x[batch_size*j:batch_size*(j+1)]\n",
    "    lables_batch= enc_batch\n",
    "        \n",
    "    cntxt= sess.run(context, feed_dict= {encoder_input: enc_batch})\n",
    "    cntxt= cntxt.reshape(-1,1,input_size)\n",
    "        \n",
    "    dec_batch= np.concatenate((cntxt,enc_batch), axis= 1)\n",
    "    dec_batch= dec_batch[:,:-1]\n",
    "        \n",
    "    cost,_= sess.run([loss,optimize],\n",
    "                        feed_dict= {encoder_input: enc_batch,\n",
    "                                        decoder_input: dec_batch,\n",
    "                                        labels: lables_batch})\n",
    "    total_loss+= cost\n",
    "        \n",
    "print ('total loss for the test part is %.3f'%(total_loss/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
